"""
New file, e.g., adforce_dataset.py
This version is updated to "lazily" read the pre-processed
NetCDF files generated by your `swegnn_netcdf_creation` function.

**MODIFIED:** This version saves its processed index map using
NetCDF/xarray instead of pickle for portability.
"""
import torch
import xarray as xr
import glob
from torch_geometric.data import Dataset, Data
from tqdm import tqdm
import os
# import pickle  <- No longer needed
import warnings
import numpy as np


class AdforceLazyDataset(Dataset):
    """
    A "lazy-loading" PyG Dataset for multiple pre-processed
    SWE-GNN NetCDF simulations.

    Assumes:
    1.  Each .nc file was created by `swegnn_netcdf_creation`.
    2.  The static mesh is IDENTICAL across all files.
    """
    def __init__(self, root, nc_files, transform=None, pre_transform=None):
        """
        Args:
            root (str): Root directory to store processed index map.
            nc_files (list[str]): The list of PRE-PROCESSED .nc files.
        """
        self.nc_files = sorted(nc_files)

        # --- Caches for static data and open file handles ---
        self._static_data = None
        self._file_handles = {}

        super().__init__(root, transform, pre_transform)

        # --- MODIFIED: Load the index map from NetCDF ---
        try:
            with xr.open_dataset(self.processed_paths[0]) as ds:
                self.total_nodes = ds.attrs['total_nodes']

                # Reconstruct the index_map from the two arrays
                file_paths = ds['file_paths'].values
                time_indices = ds['time_indices'].values

                # We use list(zip(...)) to rebuild the list of tuples
                self.index_map = list(zip(file_paths, time_indices))

        except FileNotFoundError:
            raise RuntimeError(f"Processed file not found at {self.processed_paths[0]}. Please check the 'root' directory or re-run processing.")
        except Exception as e:
            raise IOError(f"Failed to load processed index file: {e}")
        # --- End Modification ---

    @property
    def processed_file_names(self):
        """The file that will store our index map."""
        # --- MODIFIED: Use .nc instead of .pkl ---
        return ['index_map.nc']

    def process(self):
        """
        Runs ONCE. Scans all files, builds the index map,
        and verifies mesh consistency and variable presence.
        """
        print(f"Building index map for {len(self.nc_files)} files...")

        # --- Define all required variables ---
        # (Same as before)
        required_static_vars = [
            'edge_index', 'face_distance', 'edge_slope',
            'DEM', 'slopex', 'slopey', 'area'
        ]
        required_dynamic_vars = ['WX', 'WY', 'P', 'WD', 'VX', 'VY']
        all_required_vars = set(required_static_vars + required_dynamic_vars)

        # --- Caches for mesh consistency check ---
        # (Same as before)
        reference_edge_index = None
        total_nodes = None
        first_valid_file_found = False

        index_map = []
        global_idx = 0
        valid_datasets = 0

        for nc_path in tqdm(self.nc_files, desc="Processing files"):
            try:
                with xr.open_dataset(nc_path) as ds:

                    # --- New Check: Variable Presence ---
                    available_vars = set(ds.data_vars.keys())
                    if not all_required_vars.issubset(available_vars):
                        missing = all_required_vars - available_vars
                        warnings.warn(f"File {nc_path} is missing variables: {missing}. Skipping file.")
                        continue # Skip this file

                    # --- Sanity Check: Mesh Consistency ---
                    current_edge_index = torch.tensor(ds['edge_index'].values, dtype=torch.long)

                    if not first_valid_file_found:
                        reference_edge_index = current_edge_index
                        total_nodes = ds.sizes['num_nodes']
                        first_valid_file_found = True

                    elif not torch.equal(reference_edge_index, current_edge_index):
                        warnings.warn(f"Mesh mismatch in {nc_path}! Skipping file.")
                        continue # Skip this file

                    # --- If all checks pass, add to index map ---
                    num_timesteps = ds.sizes['time']

                    for t in range(num_timesteps - 1):
                        index_map.append((nc_path, t))
                        global_idx += 1
                valid_datasets += 1

            except Exception as e:
                warnings.warn(f"Failed to open or process {nc_path}: {e}. Skipping file.")
                continue # Skip corrupt or unreadable files

        if not index_map:
            raise IOError("No valid time steps found across all NetCDF files.")

        if total_nodes is None:
             raise IOError("No valid files were found to establish mesh properties.")

        print(f"Index map built. Total samples: {len(index_map)},\n Total nodes per sample: {total_nodes},\n Valid files: {valid_datasets}/{len(self.nc_files)}, {valid_datasets /len(self.nc_files)*100:.1f}%")

        # --- MODIFIED: Save the map using xarray/NetCDF ---

        # 1. "Unzip" the index map into two separate lists
        # We need to use numpy arrays for xarray
        file_paths = np.array([item[0] for item in index_map], dtype='object')
        time_indices = np.array([item[1] for item in index_map], dtype=np.int32)

        # 2. Create a dimension for the samples
        sample_dim = 'sample_idx'
        sample_coords = np.arange(len(index_map))

        # 3. Create an xarray Dataset
        ds_index = xr.Dataset(
            data_vars={
                'file_paths': (sample_dim, file_paths),
                'time_indices': (sample_dim, time_indices),
            },
            coords={
                sample_dim: sample_coords
            }
        )

        # 4. Store total_nodes as a global attribute (very common in NetCDF)
        ds_index.attrs['total_nodes'] = total_nodes

        # 5. Save to the processed file path
        try:
            # Use 'w' mode to overwrite if it already exists
            ds_index.to_netcdf(self.processed_paths[0], mode='w')
        except Exception as e:
            raise IOError(f"Failed to write processed index file: {e}")
        finally:
            ds_index.close()
        # --- End Modification ---


    def _load_static_data(self, nc_path):
        """Helper to load mesh/static data from a file."""
        # (This method is unchanged)
        with xr.open_dataset(nc_path) as ds_static:

            # --- Static Edge Features ---
            edge_index = torch.tensor(ds_static['edge_index'].values, dtype=torch.long)

            # Combine all static edge features
            static_edge_attr = torch.stack([
                torch.tensor(ds_static['face_distance'].values, dtype=torch.float),
                torch.tensor(ds_static['edge_slope'].values, dtype=torch.float)
            ], dim=1) # Shape [num_edges, 2]

            # --- Static Node Features ---
            dem = torch.tensor(ds_static['DEM'].values, dtype=torch.float)
            slopex = torch.tensor(ds_static['slopex'].values, dtype=torch.float)
            slopey = torch.tensor(ds_static['slopey'].values, dtype=torch.float)
            area = torch.tensor(ds_static['area'].values, dtype=torch.float)

            # --- Create node_type from boundary data ---
            # 0 = internal, 1 = boundary
            num_real_nodes = ds_static.dims['num_nodes']
            node_type = torch.zeros(num_real_nodes, dtype=torch.float)
            if 'face_BC' in ds_static:
                # face_BC holds the indices of faces that are on the boundary
                boundary_face_indices = torch.tensor(ds_static['face_BC'].values, dtype=torch.long)
                if boundary_face_indices.numel() > 0:
                    node_type[boundary_face_indices] = 1.0 # Mark as boundary

            static_node_features = torch.stack([
                dem,
                slopex,
                slopey,
                area,
                node_type
            ], dim=1) # Shape [num_nodes, 5]

            return {
                'edge_index': edge_index,
                'static_node_features': static_node_features,
                'static_edge_attr': static_edge_attr
            }

    def len(self):
        """Returns the total number of samples (time steps)"""
        # (This method is unchanged)
        return len(self.index_map)

    def get(self, idx):
        """
        THE "LAZY" PART.
        Loads a single (t, t+1) sample from disk.
        """
        # (This method is unchanged)

        # 1. Load static data if not already cached
        if self._static_data is None:
            # Assumes nc_files[0] is valid. The 'process' check should ensure this.
            self._static_data = self._load_static_data(self.nc_files[0])

        # 2. Find which file and time index this sample corresponds to
        nc_path, t = self.index_map[idx]

        # 3. Open the file (or get from cache)
        if nc_path not in self._file_handles:
            # Load with cache=True for potentially faster subsequent reads
            self._file_handles[nc_path] = xr.open_dataset(nc_path, cache=True)
        ds = self._file_handles[nc_path]

        # 4. Get dynamic features for time t (inputs)
        # These are your new FORCING variables
        dyn_node_features_t = torch.tensor(
            ds[['WX', 'WY', 'P']].isel(time=t).to_array().values.T,
            dtype=torch.float
        ) # Shape [num_nodes, 3]

        # 5. Get dynamic features for time t+1 (targets)
        y_tplus1 = torch.tensor(
            ds[['WD', 'VX', 'VY']].isel(time=t+1).to_array().values.T,
            dtype=torch.float
        ) # Shape [num_nodes, 3]

        # 6. Combine static and dynamic inputs
        x_t = torch.cat([
            self._static_data['static_node_features'],
            dyn_node_features_t
        ], dim=1) # Shape [num_nodes, 8]

        # 7. Construct and return the Data object
        return Data(x=x_t,
                    edge_index=self._static_data['edge_index'],
                    edge_attr=self._static_data['static_edge_attr'],
                    y=y_tplus1)

    def close(self):
        """Call this to close all open NetCDF file handles."""
        # (This method is unchanged)
        for handle in self._file_handles.values():
            handle.close()
        self._file_handles = {}
