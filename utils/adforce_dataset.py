"""
New file, e.g., adforce_dataset.py
This version is updated to "lazily" read the pre-processed
NetCDF files generated by your `swegnn_netcdf_creation` function.
"""
import torch
import xarray as xr
import glob
from torch_geometric.data import Dataset, Data
from tqdm import tqdm
import os
import pickle
import warnings
import numpy as np


class AdforceLazyDataset(Dataset):
    """
    A "lazy-loading" PyG Dataset for multiple pre-processed
    SWE-GNN NetCDF simulations.

    Assumes:
    1.  Each .nc file was created by `swegnn_netcdf_creation`.
    2.  The static mesh is IDENTICAL across all files.
    """
    def __init__(self, root, nc_files, transform=None, pre_transform=None):
        """
        Args:
            root (str): Root directory to store processed index map.
            nc_files (list[str]): The list of PRE-PROCESSED .nc files.
        """
        self.nc_files = sorted(nc_files)

        # --- Caches for static data and open file handles ---
        self._static_data = None
        self._file_handles = {}

        super().__init__(root, transform, pre_transform)

        # Load the index map
        with open(self.processed_paths[0], 'rb') as f:
            self.index_map, self.total_nodes = pickle.load(f)

    @property
    def processed_file_names(self):
        """The file that will store our index map."""
        return ['index_map.pkl']

    def process(self):
        """
        Runs ONCE. Scans all files, builds the index map,
        and verifies mesh consistency and variable presence.
        """
        print(f"Building index map for {len(self.nc_files)} files...")

        # --- Define all required variables ---
        # Based on _load_static_data
        required_static_vars = [
            'edge_index', 'face_distance', 'edge_slope',
            'DEM', 'slopex', 'slopey', 'area'
        ]
        # Based on get()
        required_dynamic_vars = ['WX', 'WY', 'P', 'WD', 'VX', 'VY']

        # Combine all. 'face_BC' is optional and handled in _load_static_data
        all_required_vars = set(required_static_vars + required_dynamic_vars)

        # --- Caches for mesh consistency check ---
        reference_edge_index = None
        total_nodes = None
        first_valid_file_found = False

        index_map = []
        global_idx = 0
        valid_datasets = 0

        for nc_path in tqdm(self.nc_files, desc="Processing files"):
            try:
                with xr.open_dataset(nc_path) as ds:

                    # --- New Check: Variable Presence ---
                    available_vars = set(ds.data_vars.keys())
                    if not all_required_vars.issubset(available_vars):
                        missing = all_required_vars - available_vars
                        warnings.warn(f"File {nc_path} is missing variables: {missing}. Skipping file.")
                        continue # Skip this file

                    # --- Sanity Check: Mesh Consistency ---
                    current_edge_index = torch.tensor(ds['edge_index'].values, dtype=torch.long)

                    if not first_valid_file_found:
                        # This is the first file that passed the variable check.
                        # Use it as the reference.
                        reference_edge_index = current_edge_index
                        total_nodes = ds.sizes['num_nodes']
                        first_valid_file_found = True

                    elif not torch.equal(reference_edge_index, current_edge_index):
                        warnings.warn(f"Mesh mismatch in {nc_path}! Skipping file.")
                        continue # Skip this file

                    # --- If all checks pass, add to index map ---
                    num_timesteps = ds.sizes['time']

                    # We can't get a target for the last time step
                    for t in range(num_timesteps - 1):
                        index_map.append((nc_path, t))
                        global_idx += 1
                valid_datasets += 1

            except Exception as e:
                warnings.warn(f"Failed to open or process {nc_path}: {e}. Skipping file.")
                continue # Skip corrupt or unreadable files

        if not index_map:
            raise IOError("No valid time steps found across all NetCDF files.")

        if total_nodes is None:
             raise IOError("No valid files were found to establish mesh properties.")

        print(f"Index map built. Total samples: {len(index_map)},\n Total nodes per sample: {total_nodes},\n Valid files: {valid_datasets}/{len(self.nc_files)}, {valid_datasets /len(self.nc_files)*100:.1f}%")

        # Save the map AND the node count
        with open(self.processed_paths[0], 'wb') as f:
            pickle.dump((index_map, total_nodes), f)

    def _load_static_data(self, nc_path):
        """Helper to load mesh/static data from a file."""
        with xr.open_dataset(nc_path) as ds_static:

            # --- Static Edge Features ---
            edge_index = torch.tensor(ds_static['edge_index'].values, dtype=torch.long)

            # Combine all static edge features
            static_edge_attr = torch.stack([
                torch.tensor(ds_static['face_distance'].values, dtype=torch.float),
                torch.tensor(ds_static['edge_slope'].values, dtype=torch.float)
            ], dim=1) # Shape [num_edges, 2]

            # --- Static Node Features ---
            dem = torch.tensor(ds_static['DEM'].values, dtype=torch.float)
            slopex = torch.tensor(ds_static['slopex'].values, dtype=torch.float)
            slopey = torch.tensor(ds_static['slopey'].values, dtype=torch.float)
            area = torch.tensor(ds_static['area'].values, dtype=torch.float)

            # --- Create node_type from boundary data ---
            # 0 = internal, 1 = boundary
            num_real_nodes = ds_static.dims['num_nodes']
            node_type = torch.zeros(num_real_nodes, dtype=torch.float)
            if 'face_BC' in ds_static:
                # face_BC holds the indices of faces that are on the boundary
                boundary_face_indices = torch.tensor(ds_static['face_BC'].values, dtype=torch.long)
                if boundary_face_indices.numel() > 0:
                    node_type[boundary_face_indices] = 1.0 # Mark as boundary

            static_node_features = torch.stack([
                dem,
                slopex,
                slopey,
                area,
                node_type
            ], dim=1) # Shape [num_nodes, 5]

            return {
                'edge_index': edge_index,
                'static_node_features': static_node_features,
                'static_edge_attr': static_edge_attr
            }

    def len(self):
        """Returns the total number of samples (time steps)"""
        return len(self.index_map)

    def get(self, idx):
        """
        THE "LAZY" PART.
        Loads a single (t, t+1) sample from disk.
        """

        # 1. Load static data if not already cached
        if self._static_data is None:
            self._static_data = self._load_static_data(self.nc_files[0])

        # 2. Find which file and time index this sample corresponds to
        nc_path, t = self.index_map[idx]

        # 3. Open the file (or get from cache)
        if nc_path not in self._file_handles:
            # Load with cache=True for potentially faster subsequent reads
            self._file_handles[nc_path] = xr.open_dataset(nc_path, cache=True)
        ds = self._file_handles[nc_path]

        # 4. Get dynamic features for time t (inputs)
        # These are your new FORCING variables
        dyn_node_features_t = torch.tensor(
            ds[['WX', 'WY', 'P']].isel(time=t).to_array().values.T,
            dtype=torch.float
        ) # Shape [num_nodes, 3]

        # 5. Get dynamic features for time t+1 (targets)
        y_tplus1 = torch.tensor(
            ds[['WD', 'VX', 'VY']].isel(time=t+1).to_array().values.T,
            dtype=torch.float
        ) # Shape [num_nodes, 3]

        # 6. Combine static and dynamic inputs
        x_t = torch.cat([
            self._static_data['static_node_features'],
            dyn_node_features_t
        ], dim=1) # Shape [num_nodes, 8]

        # 7. Construct and return the Data object
        return Data(x=x_t,
                    edge_index=self._static_data['edge_index'],
                    edge_attr=self._static_data['static_edge_attr'],
                    y=y_tplus1)

    def close(self):
        """Call this to close all open NetCDF file handles."""
        for handle in self._file_handles.values():
            handle.close()
        self._file_handles = {}
