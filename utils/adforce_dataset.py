"""
New file, e.g., adforce_dataset.py
This version is updated to "lazily" read the pre-processed
NetCDF files generated by your `swegnn_netcdf_creation` function in adforce.mesh.

**MODIFIED:** This version saves its processed index map using
NetCDF/xarray instead of pickle for portability.

**MODIFIED:** This version is updated to load `previous_t` input steps
and 1 output step, to work with the 1-step-ahead training pipeline.

SWE_GNN dataset created with variables: ['x', 'y', 'DEM', 'WD', 'VX', 'VY', 'WX', 'WY', 'P', 'slopex', 'slopey', 'area', 'edge_index', 'face_distance', 'face_relative_distance', 'edge_slope', 'element', 'edge_index_BC', 'face_BC', 'ghost_face_x', 'ghost_face_y', 'ghost_node_x', 'ghost_node_y', 'original_ghost_node_indices', 'ghost_node_indices', 'node_BC', 'edge_BC_length']

Example NetCDF structure:
netcdf swegnn {
dimensions:
        num_nodes = 58369 ;
        time = 17 ;
        two = 2 ;
        edge = 170492 ;
        xy = 2 ;
        nvertex = 3 ;
        nele = 58369 ;
        num_BC_edges = 102 ;
        num_ghost_nodes = 102 ;
variables:
        double x(num_nodes) ;
                x:_FillValue = NaN ;
                x:description = "Longitude of the dual graph nodes." ;
                x:units = "degrees_east" ;
        double y(num_nodes) ;
                y:_FillValue = NaN ;
                y:description = "Latitude of the dual graph nodes." ;
                y:units = "degrees_north" ;
        float DEM(num_nodes) ;
                DEM:_FillValue = NaNf ;
                DEM:description = "Bathymetry/DEM at face centers (m, positive up)" ;
                DEM:units = "m" ;
        float WD(time, num_nodes) ;
                WD:_FillValue = NaNf ;
                WD:description = "Water depth at face centers (time series)" ;
                WD:units = "m" ;
        float VX(time, num_nodes) ;
                VX:_FillValue = NaNf ;
                VX:description = "X-velocity at face centers (time series)" ;
                VX:units = "m/s" ;
        float VY(time, num_nodes) ;
                VY:_FillValue = NaNf ;
                VY:description = "Y-velocity at face centers (time series)" ;
                VY:units = "m/s" ;
        double WX(time, num_nodes) ;
                WX:_FillValue = NaN ;
                WX:description = "X-component of wind at face centers (time series)" ;
                WX:units = "m/s" ;
        double WY(time, num_nodes) ;
                WY:_FillValue = NaN ;
                WY:description = "Y-component of wind at face centers (time series)" ;
                WY:units = "m/s" ;
        double P(time, num_nodes) ;
                P:_FillValue = NaN ;
                P:description = "Atmospheric pressure at face centers (time series)" ;
                P:units = "m" ;
        float slopex(num_nodes) ;
                slopex:_FillValue = NaNf ;
                slopex:description = "Topographic slope in x-direction at face centers" ;
                slopex:units = "m/degree_east" ;
        float slopey(num_nodes) ;
                slopey:_FillValue = NaNf ;
                slopey:description = "Topographic slope in y-direction at face centers" ;
                slopey:units = "m/degree_north" ;
        float area(num_nodes) ;
                area:_FillValue = NaNf ;
                area:description = "Area of each mesh face (triangle)" ;
                area:units = "m^2" ;
        int edge_index(two, edge) ;
                edge_index:description = "Dual graph connectivity (face indices)" ;
                edge_index:units = "index" ;
        float face_distance(edge) ;
                face_distance:_FillValue = NaNf ;
                face_distance:description = "Distance between centers of connected faces" ;
                face_distance:units = "degrees" ;
        float face_relative_distance(edge, xy) ;
                face_relative_distance:_FillValue = NaNf ;
                face_relative_distance:description = "Vector (dx, dy) between centers of connected faces" ;
                face_relative_distance:units = "degrees" ;
        float edge_slope(edge) ;
                edge_slope:_FillValue = NaNf ;
                edge_slope:description = "Slope between connected faces based on DEM" ;
                edge_slope:units = "m/degree" ;
        int64 edge(edge) ;
        int64 num_nodes(num_nodes) ;
        int64 nvertex(nvertex) ;
        int64 time(time) ;
                time:units = "hours since 2005-08-25 00:16:40" ;
                time:calendar = "proleptic_gregorian" ;
        int64 nele(nele) ;
        int element(nele, nvertex) ;
                element:description = "Original mesh triangles, with each new node corresponding to the face of the old triangle mesh." ;
        string two(two) ;
        int64 num_BC_edges(num_BC_edges) ;
        int edge_index_BC(num_BC_edges, two) ;
        int face_BC(num_BC_edges) ;
        float ghost_face_x(num_BC_edges) ;
                ghost_face_x:_FillValue = NaNf ;
        float ghost_face_y(num_BC_edges) ;
                ghost_face_y:_FillValue = NaNf ;
        int64 num_ghost_nodes(num_ghost_nodes) ;
        float ghost_node_x(num_ghost_nodes) ;
                ghost_node_x:_FillValue = NaNf ;
        float ghost_node_y(num_ghost_nodes) ;
                ghost_node_y:_FillValue = NaNf ;
        int original_ghost_node_indices(num_ghost_nodes) ;
        int ghost_node_indices(num_ghost_nodes) ;
        int node_BC(num_BC_edges) ;
                node_BC:description = "Indices assigned to ghost cells (faces) appended after real faces" ;
        float edge_BC_length(num_BC_edges) ;
                edge_BC_length:_FillValue = NaNf ;

// global attributes:
                :description = "Dual graph formatted for mSWE-GNN input pipeline" ;
}

"""
import torch
import xarray as xr
from torch_geometric.data import Dataset, Data
from tqdm import tqdm
import warnings
import numpy as np

# import os
# import pickle  <- No longer needed\
# import glob


class AdforceLazyDataset(Dataset):
    """
    A "lazy-loading" PyG Dataset for multiple pre-processed
    SWE-GNN NetCDF simulations.

    Assumes:
    1.  Each .nc file was created by `swegnn_netcdf_creation`.
    2.  The static mesh is IDENTICAL across all files.
    3.  This loader provides 1-step-ahead data (rollout_steps=1).
    """
    def __init__(self, root, nc_files, previous_t, transform=None, pre_transform=None):
        """
        Args:
            root (str): Root directory to store processed index map.
            nc_files (list[str]): The list of PRE-PROCESSED .nc files.
            previous_t (int): Number of input time steps.
        """
        self.nc_files = sorted(nc_files)

        # --- MODIFIED: Set window parameters ---
        self.previous_t = previous_t
        self.rollout_steps = 1 # Hard-coded for 1-step-ahead training
        # --- End Modification ---

        # --- Caches for static data and open file handles ---
        self._static_data = None
        self._file_handles = {}
        self.total_nodes = None # Will be loaded from processed file

        super().__init__(root, transform, pre_transform)

        # --- MODIFIED: Load the index map from NetCDF ---
        try:
            with xr.open_dataset(self.processed_paths[0]) as ds:
                self.total_nodes = ds.attrs['total_nodes']

                # --- MODIFIED: Verify window parameters ---
                loaded_p_t = ds.attrs.get('previous_t', 1) # Default to 1 for old files
                loaded_r_s = ds.attrs.get('rollout_steps', 1) # Default to 1 for old files

                if loaded_p_t != self.previous_t or loaded_r_s != self.rollout_steps:
                    raise ValueError(
                        f"Window mismatch! Dataset file was processed with "
                        f"previous_t={loaded_p_t} and rollout_steps={loaded_r_s}, "
                        f"but {self.previous_t} and {self.rollout_steps} were requested. "
                        f"Delete the 'processed' directory (e.g., '{self.processed_dir}') and re-run."
                    )
                # --- End Modification ---

                # Reconstruct the index_map from the two arrays
                file_paths = ds['file_paths'].values
                time_indices = ds['time_indices'].values

                # We use list(zip(...)) to rebuild the list of tuples
                self.index_map = list(zip(file_paths, time_indices))

        except FileNotFoundError:
            raise RuntimeError(f"Processed file not found at {self.processed_paths[0]}. Please check the 'root' directory or re-run processing.")
        except Exception as e:
            raise IOError(f"Failed to load processed index file: {e}")
        # --- End Modification ---

    @property
    def processed_file_names(self):
        """The file that will store our index map."""
        # --- MODIFIED: Use .nc and add window parameters to filename ---
        return [f'index_map_p{self.previous_t}_r{self.rollout_steps}.nc']

    def process(self):
        """
        Runs ONCE. Scans all files, builds the index map,
        and verifies mesh consistency and variable presence.
        """
        print(f"Building index map for {len(self.nc_files)} files (p_t={self.previous_t}, r_s={self.rollout_steps})...")

        # --- Define all required variables ---
        required_static_vars = [
            'edge_index', 'face_distance', 'edge_slope',
            'DEM', 'slopex', 'slopey', 'area',
            'node_BC'
        ]
        required_dynamic_vars = ['WX', 'WY', 'P', 'WD', 'VX', 'VY']
        all_required_vars = set(required_static_vars + required_dynamic_vars)

        # --- Caches for mesh consistency check ---
        reference_edge_index = None
        total_nodes = None
        first_valid_file_found = False

        index_map = []
        global_idx = 0
        valid_datasets = 0

        for nc_path in tqdm(self.nc_files, desc="Processing files"):
            try:
                with xr.open_dataset(nc_path) as ds:

                    # --- New Check: Variable Presence ---
                    available_vars = set(ds.data_vars.keys())
                    if not all_required_vars.issubset(available_vars):
                        missing = all_required_vars - available_vars
                        warnings.warn(f"File {nc_path} is missing variables: {missing}. Skipping file.")
                        continue # Skip this file

                    # --- Sanity Check: Mesh Consistency ---
                    current_edge_index = torch.tensor(ds['edge_index'].values, dtype=torch.long)

                    if not first_valid_file_found:
                        reference_edge_index = current_edge_index
                        total_nodes = ds.sizes['num_nodes']
                        first_valid_file_found = True

                    elif not torch.equal(reference_edge_index, current_edge_index):
                        warnings.warn(f"Mesh mismatch in {nc_path}! Skipping file.")
                        continue # Skip this file

                    # --- If all checks pass, add to index map ---
                    num_timesteps = ds.sizes['time']

                    # --- MODIFIED: Account for full input/output window ---
                    # We need `previous_t` steps for input and `rollout_steps` for output
                    # The last valid START index `t` is (num_timesteps - previous_t - rollout_steps)
                    valid_steps = num_timesteps - self.previous_t - self.rollout_steps + 1

                    for t in range(valid_steps):
                        index_map.append((nc_path, t))
                        global_idx += 1
                valid_datasets += 1

            except Exception as e:
                warnings.warn(f"Failed to open or process {nc_path}: {e}. Skipping file.")
                continue # Skip corrupt or unreadable files

        if not index_map:
            raise IOError("No valid time steps found across all NetCDF files.")

        if total_nodes is None:
             raise IOError("No valid files were found to establish mesh properties.")

        print(f"Index map built. Total samples: {len(index_map)},\n Total nodes per sample: {total_nodes},\n Valid files: {valid_datasets}/{len(self.nc_files)}, {valid_datasets /len(self.nc_files)*100:.1f}%")

        # --- MODIFIED: Save the map using xarray/NetCDF ---
        file_paths = np.array([item[0] for item in index_map], dtype='object')
        time_indices = np.array([item[1] for item in index_map], dtype=np.int32)

        sample_dim = 'sample_idx'
        sample_coords = np.arange(len(index_map))

        ds_index = xr.Dataset(
            data_vars={
                'file_paths': (sample_dim, file_paths),
                'time_indices': (sample_dim, time_indices),
            },
            coords={
                sample_dim: sample_coords
            }
        )

        # 4. Store metadata as global attributes
        ds_index.attrs['total_nodes'] = total_nodes
        ds_index.attrs['previous_t'] = self.previous_t
        ds_index.attrs['rollout_steps'] = self.rollout_steps

        # 5. Save to the processed file path
        try:
            ds_index.to_netcdf(self.processed_paths[0], mode='w')
        except Exception as e:
            raise IOError(f"Failed to write processed index file: {e}")
        finally:
            ds_index.close()
        # --- End Modification ---

    def _load_static_data(self, nc_path):
        """Helper to load mesh/static data from a file."""
        with xr.open_dataset(nc_path) as ds_static:

            # --- (edge_index and static_edge_attr logic is unchanged) ---
            edge_index = torch.tensor(ds_static['edge_index'].values, dtype=torch.long)
            static_edge_attr = torch.stack([
                torch.tensor(ds_static['face_distance'].values, dtype=torch.float),
                torch.tensor(ds_static['edge_slope'].values, dtype=torch.float)
            ], dim=1)

            # --- (static_node_features logic is unchanged) ---
            dem = torch.tensor(ds_static['DEM'].values, dtype=torch.float)
            slopex = torch.tensor(ds_static['slopex'].values, dtype=torch.float)
            slopey = torch.tensor(ds_static['slopey'].values, dtype=torch.float)
            area = torch.tensor(ds_static['area'].values, dtype=torch.float)

            # --- MODIFICATION: Store node_BC indices ---
            num_real_nodes = ds_static.dims['num_nodes']
            node_type = torch.zeros(num_real_nodes, dtype=torch.float)
            boundary_face_indices = torch.tensor([], dtype=torch.long) # Default empty tensor

            if 'face_BC' in ds_static:
                # face_BC holds the indices of faces that are on the boundary
                boundary_face_indices = torch.tensor(ds_static['face_BC'].values, dtype=torch.long)
                if boundary_face_indices.numel() > 0:
                    node_type[boundary_face_indices] = 1.0 # Mark as boundary
            # --- End Modification ---

            static_node_features = torch.stack([
                dem, slopex, slopey, area, node_type
            ], dim=1) # Shape [num_nodes, 5]

            # --- MODIFICATION: Return node_BC as well ---
            return {
                'edge_index': edge_index,
                'static_node_features': static_node_features,
                'static_edge_attr': static_edge_attr,
                'node_BC': boundary_face_indices
            }

    def len(self):
        """Returns the total number of samples (time steps)"""
        return len(self.index_map)

    def get(self, idx):
        """
        Loads a single (t, t+1) sample from disk.
        """
        # 1. Load static data if not already cached
        if self._static_data is None:
            self._static_data = self._load_static_data(self.nc_files[0])
            if self.total_nodes is None:
                 self.total_nodes = self._static_data['static_node_features'].shape[0]

        # ... (Steps 2 & 3: Find file and open handle are unchanged) ...
        nc_path, t_start = self.index_map[idx]
        if nc_path not in self._file_handles:
            self._file_handles[nc_path] = xr.open_dataset(nc_path, cache=True)
        ds = self._file_handles[nc_path]

        try:
            # ... (Steps 4 & 5: Load input/output slices are unchanged) ...
            in_start = t_start
            in_end = t_start + self.previous_t
            dyn_node_features_t_raw = torch.tensor(
                ds[['WX', 'WY', 'P']].isel(time=slice(in_start, in_end)).to_array().values,
                dtype=torch.float
            )
            num_nodes = dyn_node_features_t_raw.shape[1]
            dyn_node_features_t = dyn_node_features_t_raw.permute(1, 2, 0).reshape(
                num_nodes, -1
            )

            target_time_idx = t_start + self.previous_t
            y_tplus1_raw = torch.tensor(
                ds[['WD', 'VX', 'VY']].isel(time=target_time_idx).to_array().values,
                dtype=torch.float
            )
            y_tplus1 = y_tplus1_raw.T

            # 6. Combine static and dynamic inputs
            x_t = torch.cat([
                self._static_data['static_node_features'],
                dyn_node_features_t
            ], dim=1)

            # --- MODIFICATION: Add node_BC to the Data object ---
            return Data(x=x_t,
                        edge_index=self._static_data['edge_index'],
                        edge_attr=self._static_data['static_edge_attr'],
                        y=y_tplus1,
                        node_BC=self._static_data['node_BC']) # Add this line
            # --- End Modification ---

        except Exception as e:
            raise IOError(f"Error loading sample {idx} (file: {nc_path}, time_idx: {t_start}): {e}")

    def close(self):
        """Call this to close all open NetCDF file handles."""
        # (This method is unchanged)
        for handle in self._file_handles.values():
            handle.close()
        self._file_handles = {}
